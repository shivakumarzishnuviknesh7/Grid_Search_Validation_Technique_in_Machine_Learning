# -*- coding: utf-8 -*-
"""grid_search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jIDdjL1lXCvJwfn22ZkpChMZeifwAj7S

# Grid Search

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Training the Kernel SVM model on the Training set

# Note

Differences Between Parameters and Hyperparameters:
## Parameters:

Learned from the data during the training process.
Examples include weights in neural networks, coefficients in linear regression, and support vectors in SVM.

## Hyperparameters:

Set before the training process.
Examples include learning rate, number of hidden layers in a neural network, and regularization strength.
"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0) #hyper parameters
classifier.fit(X_train, y_train)

"""## Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""## Applying k-Fold Cross Validation"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""## Applying Grid Search to find the best model and the best parameters"""

#to find optimal values for hyper paramenter for any models to get high accuracy we use grid search
from sklearn.model_selection import GridSearchCV

# gama parameter which we want to tune can only be used with the rbf kernel and not with the linear kernel if it could be used with linear kernel then no nedd to make 2 dictionaries only 1 is enough
parameters = [{'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear']},
              {'C': [0.25, 0.5, 0.75, 1], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]
grid_search = GridSearchCV(estimator = classifier, # kernel scm classifier
                           param_grid = parameters, # our parameter list of hyper parameter values
                           scoring = 'accuracy',
                           cv = 10, # it will be evaluavted in kfold so weg ave 10 trained test polls kfold which we apply to each of the combinations
                           n_jobs = -1) # to set how to run the processor in the machine we kept -1 which means all processor will be used (to make optimised execution)
grid_search.fit(X_train, y_train)
best_accuracy = grid_search.best_score_
print("************************************************************************************************************************************************")
print(best_accuracy)
print("************************************************************************************************************************************************")
best_parameters = grid_search.best_params_
print("************************************************************************************************************************************************")
print(best_parameters)
print("************************************************************************************************************************************************")
print("Best Accuracy: {:.2f} %".format(best_accuracy*100))
print("Best Parameters:", best_parameters)

"""## Visualising the Training set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Kernel SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""## Visualising the Test set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Kernel SVM (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()